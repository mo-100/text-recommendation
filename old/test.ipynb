{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341bfb34574b5bc2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ec5b8b85807eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "import faiss\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from keybert import KeyBERT\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be70ff1b64b6408",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8a7b4967f0c3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentGraph:\n",
    "    \"\"\"Manages the content relationship graph\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.topic_threshold = 0.3\n",
    "        self.graph = nx.Graph()\n",
    "\n",
    "    def add_node(self, content_id: str, attributes=None):\n",
    "        \"\"\"Add a node to the graph with optional attributes\"\"\"\n",
    "        self.graph.add_node(content_id, **(attributes or {}))\n",
    "\n",
    "    def connect_by_entities(self, content_id: str, other_id: str, weight: float):\n",
    "        \"\"\"Connect two content pieces based on shared entities\"\"\"\n",
    "        if weight > 0:\n",
    "            if self.graph.has_edge(content_id, other_id):\n",
    "                self.graph[content_id][other_id]['weight'] += weight\n",
    "            else:\n",
    "                self.graph.add_edge(content_id, other_id, weight=weight)\n",
    "\n",
    "    def connect_by_topics(self, content_id: str, other_id: str, topic_similarity: float):\n",
    "        \"\"\"Connect two content pieces based on topic similarity\"\"\"\n",
    "        if topic_similarity > self.topic_threshold:  # Threshold for connection\n",
    "            if self.graph.has_edge(content_id, other_id):\n",
    "                self.graph[content_id][other_id]['weight'] += topic_similarity\n",
    "            else:\n",
    "                self.graph.add_edge(content_id, other_id, weight=topic_similarity)\n",
    "\n",
    "    def get_neighbors(self, content_id: str):\n",
    "        \"\"\"Get neighboring content for a given content ID\"\"\"\n",
    "        if content_id in self.graph:\n",
    "            return list(self.graph.neighbors(content_id))\n",
    "        return []\n",
    "\n",
    "    def get_edge_weight(self, content_id: str, other_id: str):\n",
    "        \"\"\"Get the weight of an edge between two content pieces\"\"\"\n",
    "        if self.graph.has_edge(content_id, other_id):\n",
    "            return self.graph[content_id][other_id].get('weight', 0)\n",
    "        return 0\n",
    "\n",
    "    def get_centrality(self):\n",
    "        \"\"\"Calculate degree centrality for all nodes\"\"\"\n",
    "        return nx.degree_centrality(self.graph)\n",
    "\n",
    "\n",
    "class Index:\n",
    "    def __init__(self):\n",
    "        self.index = faiss.IndexHNSWFlat(1, 1)\n",
    "\n",
    "    def add(self, embeddings: np.ndarray):\n",
    "        self.index = faiss.IndexHNSWFlat(embeddings.shape[1], 32)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def get_items_by_index(self, query_embeddings: list[np.ndarray], top_k: int) -> list[list[str]]:\n",
    "        distances, indices = self.index.search(np.array(query_embeddings), top_k)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6421ac25c913e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    user_id: str\n",
    "    interaction_history: list[str]\n",
    "    embedding_profile: np.ndarray\n",
    "    topic_interests: np.ndarray\n",
    "    entity_interests: Counter\n",
    "\n",
    "\n",
    "class Content(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    content_id: str\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "    entities: Counter\n",
    "    keywords: set[str]\n",
    "    topic_dist: np.ndarray\n",
    "    topic: int\n",
    "    centrality: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdf15f4ca734bc",
   "metadata": {},
   "source": [
    "# DB functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68d0d3513267e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_article_to_graph(content: 'Content', candidates: list['Content'], content_graph: ContentGraph):\n",
    "    # Add to content graph\n",
    "    content_graph.add_node(content.content_id)\n",
    "    similarity_matrix = cosine_similarity(\n",
    "        [c.topic_dist for c in candidates],\n",
    "        [content.topic_dist]\n",
    "    )\n",
    "\n",
    "    # Connect with existing content through shared entities\n",
    "    for i, other_content in enumerate(candidates):\n",
    "        other_id = other_content.content_id\n",
    "        if other_id == content.content_id:\n",
    "            continue\n",
    "\n",
    "        weight = 0\n",
    "        shared_entities = other_content.entities.keys() & content.entities.keys()\n",
    "        for entity in shared_entities:\n",
    "            weight += min(other_content.entities[entity], content.entities[entity])\n",
    "\n",
    "        content_graph.connect_by_entities(content.content_id, other_id, weight)\n",
    "\n",
    "        topic_similarity = similarity_matrix[i]\n",
    "\n",
    "        content_graph.connect_by_topics(content.content_id, other_id, topic_similarity)\n",
    "\n",
    "\n",
    "def create_user(user_id: str, user_profiles: dict[str, UserProfile]):\n",
    "    \"\"\"Create a new user profile\"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        user_profiles[user_id] = UserProfile(user_id=user_id, embedding_profile=[], topic_interests=[], entity_interests=Counter(), interaction_history=[])\n",
    "    return user_profiles[user_id]\n",
    "\n",
    "\n",
    "def record_interaction(user_profile: UserProfile, content: Content, history: list[Content]):\n",
    "    \"\"\"Record a user's interaction with content\"\"\"\n",
    "    user_profile.interaction_history.append(content.content_id)\n",
    "\n",
    "    # Update entity interests\n",
    "    for entity, count in content.entities.items():\n",
    "        user_profile.entity_interests[entity] += count\n",
    "\n",
    "    # Update embedding profile and topic interests\n",
    "\n",
    "    # set zeros\n",
    "    user_profile.embedding_profile = np.zeros_like(content.embedding)\n",
    "    user_profile.topic_interests = np.zeros_like(content.topic_dist)\n",
    "\n",
    "    history = history[-20:]  # Consider last 20 interactions\n",
    "    for i, h_content in enumerate(history):\n",
    "        weight = (20 - i) / 20\n",
    "        user_profile.embedding_profile += h_content.embedding * weight\n",
    "        user_profile.topic_interests +=h_content.topic_dist * weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73ba3c698aa783",
   "metadata": {},
   "source": [
    "# Similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b2b0e5ed4f4fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article - Article\n",
    "def get_embedding_similarity(content: Content, candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    similarity_matrix = cosine_similarity(\n",
    "        [c.embedding for c in candidates],\n",
    "        [content.embedding]\n",
    "    )\n",
    "\n",
    "    for i, other_content in enumerate(candidates):\n",
    "        if other_content.content_id == content.content_id:\n",
    "            continue\n",
    "        emb_similarity = similarity_matrix[i]\n",
    "        scores[other_content.content_id] = emb_similarity * weight\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_entity_similarity(neighbors: list[Content], weights: list[float], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    for neighbor, weight_ in zip(neighbors, weights):\n",
    "        scores[neighbor.content_id] = min(weight_ / 5, 1) * weight\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_topic_similarity(content: Content, candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    topic_similarity_matrix = cosine_similarity(\n",
    "        [c.topic_dist for c in candidates],\n",
    "        [content.topic_dist]\n",
    "    )\n",
    "\n",
    "    for i, other_content in enumerate(candidates):\n",
    "        if other_content.content_id == content.content_id:\n",
    "            continue\n",
    "        topic_similarity = topic_similarity_matrix[i]\n",
    "        scores[other_content.content_id] = topic_similarity * weight\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_popularity_scores(candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    for c in candidates:\n",
    "        scores[c.content_id] = c.centrality * weight\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# User - Article\n",
    "def get_embedding_similarity_user(user_profile: UserProfile, candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    similarity_matrix = cosine_similarity(\n",
    "        [c.embedding for c in candidates],\n",
    "        [user_profile.embedding_profile]\n",
    "    )\n",
    "    for i, other_content in enumerate(candidates):\n",
    "        emb_similarity = similarity_matrix[i]\n",
    "        scores[other_content.content_id] = emb_similarity * weight\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_topic_similarity_user(user_profile: UserProfile, candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    similarity_matrix = cosine_similarity(\n",
    "        [c.topic_dist for c in candidates],\n",
    "        [user_profile.topic_interests],\n",
    "    )\n",
    "    for i, content in enumerate(candidates):\n",
    "        topic_similarity = similarity_matrix[i]\n",
    "        scores[content.content_id] = topic_similarity * weight\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_entity_similarity_user(user_profile: UserProfile, candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "\n",
    "    for content in candidates:\n",
    "        content_id = content.content_id\n",
    "\n",
    "        # Calculate entity overlap score\n",
    "        score = 0\n",
    "        shared_entities = user_profile.entity_interests & content.entities\n",
    "        for entity in shared_entities:\n",
    "            # Weight by how important this entity is to the user\n",
    "            user_weight = user_profile.entity_interests[entity] / sum(user_profile.entity_interests.values())\n",
    "            # And by how important it is to the content\n",
    "            content_weight = content.entities[entity] / sum(content.entities.values())\n",
    "            score += user_weight * content_weight\n",
    "\n",
    "        if score > 0:\n",
    "            scores[content_id] = min(score * 5 * weight, 1.0)  # Normalize and apply weight\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_explore_scores_user(user_profile: UserProfile, candidates: list[Content], weight: float) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    # Get all entities user has engaged with\n",
    "    user_entities = set(user_profile.entity_interests.keys())\n",
    "    # For each content, calculate discovery score\n",
    "    for content in candidates:\n",
    "        content_id = content.content_id\n",
    "        # Skip recently viewed content\n",
    "        if any(i == content_id for i in user_profile.interaction_history[-10:]):\n",
    "            continue\n",
    "\n",
    "        # Get entities in this content\n",
    "        content_entities = set(content.entities.keys())\n",
    "\n",
    "        # Calculate novelty (% of entities not seen before)\n",
    "        if content_entities:\n",
    "            new_entities = content_entities - user_entities\n",
    "            novelty = len(new_entities) / len(content_entities)\n",
    "\n",
    "            # We want some novelty but not too much (sweet spot around 50-80% new)\n",
    "            discovery_score = 1.0 - abs(0.7 - novelty)\n",
    "\n",
    "            # But also consider content popularity via graph centrality\n",
    "\n",
    "            # Combine novelty with popularity\n",
    "            scores[content_id] = (discovery_score * 0.7 + content.centrality * 0.3) * weight\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a5d3b458f78a8",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd6741d219464d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_article(content: Content, content_store: dict[str, Content], content_graph: ContentGraph):\n",
    "    candidates = list(content_store.values())\n",
    "    neighbors = content_graph.get_neighbors(content.content_id)\n",
    "    neighbor_weights = [content_graph.get_edge_weight(content.content_id, n_id) for n_id in neighbors]\n",
    "    return candidates, neighbors, neighbor_weights\n",
    "\n",
    "def get_recommendations_article(content: Content, strategy_weights: dict[str, float], candidates: list[Content], neighbors: list[Content], neighbor_weights: list[float], num_recommendations=5):\n",
    "    \"\"\"Generate recommendations for a given content piece\"\"\"\n",
    "    # re-scoring\n",
    "    scores = {}\n",
    "\n",
    "    scores_1 = get_embedding_similarity(content, candidates, strategy_weights.get('embedding', 0.4))\n",
    "    scores_2 = get_entity_similarity(neighbors, neighbor_weights, strategy_weights.get('entity', 0.3))\n",
    "    scores_3 = get_topic_similarity(content, candidates, strategy_weights.get('topic', 0.4))\n",
    "    scores_4 = get_popularity_scores(candidates, strategy_weights.get('explore', 0.1))\n",
    "\n",
    "    for scores_dict in [scores_1, scores_2, scores_3, scores_4]:\n",
    "        for content_id, score in scores_dict.items():\n",
    "            scores[content_id] = scores.get(content_id, 0) + score\n",
    "\n",
    "\n",
    "    sorted_recommendations = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_recommendations[:num_recommendations]\n",
    "\n",
    "\n",
    "def explain_recommendation_article(content: Content, rec: Content):\n",
    "    \"\"\"Provide an explanation for why something was recommended\"\"\"\n",
    "\n",
    "    explanations = []\n",
    "\n",
    "    # Check for shared entities\n",
    "    shared_entities = content.entities.keys() & rec.entities.keys()\n",
    "    if shared_entities:\n",
    "        top_shared = sorted(shared_entities,\n",
    "                            key=lambda e: content.entities[e] + rec.entities[e],\n",
    "                            reverse=True\n",
    "                            )[:3]\n",
    "        if top_shared:\n",
    "            explanations.append(f\"Shares topics: {', '.join(top_shared)}\")\n",
    "\n",
    "    # Check for embedding similarity\n",
    "    emb_similarity = cosine_similarity(\n",
    "        [content.embedding],\n",
    "        [rec.embedding]\n",
    "    )[0][0]\n",
    "\n",
    "    if emb_similarity > 0.7:\n",
    "        explanations.append(\"Content is semantically similar\")\n",
    "    elif emb_similarity > 0.5:\n",
    "        explanations.append(\"Content is somewhat related\")\n",
    "\n",
    "    # Check for BERTopic topic similarity\n",
    "    topic_sim = cosine_similarity(\n",
    "        [content.topic_dist],\n",
    "        [rec.topic_dist]\n",
    "    )[0][0]\n",
    "\n",
    "    if topic_sim > 0.8:\n",
    "        explanations.append(\"Covers very similar topics\")\n",
    "\n",
    "    # Check if they share the same dominant topic\n",
    "    if content.topic == rec.topic and content.topic != -1:  # -1 is BERTopic's outlier topic\n",
    "        explanations.append(\"Part of the same topic cluster\")\n",
    "\n",
    "    if rec.centrality > 0.7:\n",
    "        explanations.append(\"Popular content that connects many topics\")\n",
    "\n",
    "    # Default explanation if nothing else\n",
    "    if not explanations:\n",
    "        explanations.append(\"No Reason\")\n",
    "\n",
    "    return explanations\n",
    "\n",
    "\n",
    "def get_recommendations_user(user_profile: UserProfile, strategy_weights: dict[str, float], candidates: list[Content], num_recommendations=10):\n",
    "    \"\"\"Generate personalized recommendations for user homepage\"\"\"\n",
    "    # Initialize scores\n",
    "    scores = {}\n",
    "\n",
    "    scores_1 = get_embedding_similarity_user(user_profile, candidates, strategy_weights.get('embedding', 0.4))\n",
    "    scores_2 = get_entity_similarity_user(user_profile, candidates, strategy_weights.get('entity', 0.3))\n",
    "    scores_3 = get_topic_similarity_user(user_profile, candidates, strategy_weights.get('topic', 0.4))\n",
    "    scores_4 = get_explore_scores_user(user_profile, candidates, strategy_weights.get('explore', 0.1))\n",
    "\n",
    "    for scores_dict in [scores_1, scores_2, scores_3, scores_4]:\n",
    "        for content_id, score in scores_dict.items():\n",
    "            scores[content_id] = scores.get(content_id, 0) + score\n",
    "\n",
    "    sorted_recommendations = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_recommendations[:num_recommendations]\n",
    "\n",
    "    # Filter out recently viewed content (avoid immediate repeats)\n",
    "    # recent_views = set(user_profile.interaction_history[-5:])\n",
    "    # filtered_scores = {cid: score for cid, score in scores.items() if cid not in recent_views}\n",
    "    #\n",
    "    # # If we filtered too aggressively, restore some items\n",
    "    # if len(filtered_scores) < num_recommendations / 2:\n",
    "    #     filtered_scores = scores\n",
    "    #\n",
    "    # # Sort and return top recommendations\n",
    "    # sorted_recommendations = sorted(filtered_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # # Ensure diversity by sampling from different score ranges\n",
    "    # result = sorted_recommendations[:int(num_recommendations * 0.7)]  # 70% top picks\n",
    "    #\n",
    "    # # Add 30% semi-random picks from the rest\n",
    "    # if len(sorted_recommendations) > num_recommendations:\n",
    "    #     mid_range = sorted_recommendations[int(num_recommendations * 0.7):int(len(sorted_recommendations) * 0.5)]\n",
    "    #     if mid_range:\n",
    "    #         random_picks = random.sample(mid_range, min(len(mid_range), int(num_recommendations * 0.3)))\n",
    "    #         result.extend(random_picks)\n",
    "\n",
    "    # return sorted_recommendations[:num_recommendations]\n",
    "\n",
    "\n",
    "def explain_recommendation_user(user_profile: UserProfile, rec_content: Content, history: list[Content]):\n",
    "    \"\"\"Explain why content was recommended to a user\"\"\"\n",
    "    explanations = []\n",
    "\n",
    "    # Check for embedding similarity\n",
    "    if user_profile.embedding_profile is not None:\n",
    "        emb_similarity = cosine_similarity(\n",
    "            [user_profile.embedding_profile],\n",
    "            [rec_content.embedding]\n",
    "        )[0][0]\n",
    "\n",
    "        if emb_similarity > 0.7:\n",
    "            explanations.append(\"Based on content you've engaged with\")\n",
    "        elif emb_similarity > 0.5:\n",
    "            explanations.append(\"Similar to content you've viewed\")\n",
    "\n",
    "    # Check for topic similarity\n",
    "    if user_profile.topic_interests is not None:\n",
    "        topic_sim = cosine_similarity(\n",
    "            [user_profile.topic_interests],\n",
    "            [rec_content.topic_dist]\n",
    "        )[0][0]\n",
    "\n",
    "        if topic_sim > 0.7:\n",
    "            explanations.append(\"Matches topics you're interested in\")\n",
    "            for content in history:\n",
    "\n",
    "                # Check for shared entities\n",
    "                base_entities = set(content.entities.keys())\n",
    "                rec_entities = set(rec_content.entities.keys())\n",
    "                shared_entities = base_entities.intersection(rec_entities)\n",
    "\n",
    "                if shared_entities:\n",
    "                    top_shared = sorted(shared_entities,\n",
    "                                        key=lambda e: content.entities[e] +\n",
    "                                                      rec_content.entities[e],\n",
    "                                        reverse=True)[:3]\n",
    "                    if top_shared:\n",
    "                        explanations.append(f\"Shares topics: {', '.join(top_shared)}\")\n",
    "        elif topic_sim > 0.5:\n",
    "            explanations.append(\"Related to topics you follow\")\n",
    "\n",
    "    # Check for shared entities\n",
    "    user_entities = set(user_profile.entity_interests.keys())\n",
    "    content_entities = set(rec_content.entities.keys())\n",
    "    shared_entities = user_entities.intersection(content_entities)\n",
    "\n",
    "    if shared_entities:\n",
    "        top_shared = sorted(shared_entities,\n",
    "                            key=lambda e: user_profile.entity_interests[e],\n",
    "                            reverse=True)[:2]\n",
    "        if top_shared:\n",
    "            explanations.append(f\"Mentions {', '.join(top_shared)}\")\n",
    "\n",
    "    # Check if it's popular content\n",
    "    if rec_content.centrality > 0.6:\n",
    "        explanations.append(\"Popular in your topic areas\")\n",
    "\n",
    "    # Default explanation\n",
    "    if not explanations:\n",
    "        explanations.append(\"Recommended based on your reading history\")\n",
    "\n",
    "    return explanations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891344ea402878a2",
   "metadata": {},
   "source": [
    "# Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d4bdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "def extract_keywords(texts: list[str]):\n",
    "  \"\"\"\n",
    "  Extracts keywords from text using the RAKE algorithm.\n",
    "\n",
    "  Args:\n",
    "    text: The input text string.\n",
    "\n",
    "  Returns:\n",
    "    A list of ranked keywords and phrases.\n",
    "  \"\"\"\n",
    "  r = Rake()\n",
    "  res = []\n",
    "  for t in texts:\n",
    "    r.extract_keywords_from_text(t)\n",
    "    res.append(r.get_ranked_phrases())\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b329b709875c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_ALLOWED_ENTITIES = {\n",
    "    'DATE',\n",
    "    'TIME',\n",
    "    'PERCENT',\n",
    "    'MONEY',\n",
    "    'QUANTITY',\n",
    "    'ORDINAL',\n",
    "    'CARDINAL',\n",
    "}\n",
    "\n",
    "\n",
    "def extract_entities(texts: list[str], nlp: spacy.language.Language) -> list[Counter]:\n",
    "    \"\"\"Extract entities from text documents\"\"\"\n",
    "    res = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        entities = [ent.text.lower() for ent in doc.ents if ent.label_ not in NOT_ALLOWED_ENTITIES]\n",
    "        entity_counter = Counter(entities)\n",
    "        res.append(entity_counter)\n",
    "    return res\n",
    "\n",
    "\n",
    "def extract_keywords(texts: list[str], kw_model: KeyBERT) -> list[list[str]]:\n",
    "    \"\"\"Extract entities from text documents\"\"\"\n",
    "    return [[] for _ in texts]\n",
    "    # keywords = kw_model.extract_keywords(texts, keyphrase_ngram_range=(3, 5), stop_words='english', top_n=5)\n",
    "    # for i, keywords_i in enumerate(keywords):\n",
    "    #     keywords[i] = [k[0] for k in keywords_i]\n",
    "    # return keywords\n",
    "\n",
    "\n",
    "def train_topic_model(texts: list[str], topic_model: BERTopic, embeddings: np.ndarray) -> list[int]:\n",
    "    topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "    outlier_count = topics.count(-1)\n",
    "    print(f'outlier count: {outlier_count}, outlier percentage: {outlier_count * 100 / len(texts)}')\n",
    "\n",
    "    if -1 not in topic_model.topic_sizes_:\n",
    "        return topics\n",
    "    # reduce topic outliers\n",
    "    print(f'reducing topic outliers')\n",
    "    new_topics = topic_model.reduce_outliers(texts, topics, strategy=\"c-tf-idf\", threshold=0.2)\n",
    "\n",
    "    new_outlier_count = new_topics.count(-1)\n",
    "    print(f'outlier count: {new_outlier_count}, outlier percentage: {new_outlier_count * 100 / len(texts)}')\n",
    "\n",
    "    new_topics = topic_model.reduce_outliers(texts, new_topics, strategy=\"embeddings\", embeddings=embeddings,\n",
    "                                                  threshold=0.3)\n",
    "\n",
    "    new_outlier_count = new_topics.count(-1)\n",
    "    print(f'outlier count: {new_outlier_count}, outlier percentage: {new_outlier_count * 100 / len(texts)}')\n",
    "\n",
    "    print(f'updating topics')\n",
    "    topic_model.update_topics(texts, topics=new_topics)\n",
    "    return new_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82afede24fff035",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 37038 short samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "171806"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "dataset = []\n",
    "with open('news.json') as f:\n",
    "    for line in f.readlines():\n",
    "        dataset.append(json.loads(line))\n",
    "\n",
    "num = sum(1 for d in dataset if len(d['short_description']) < 40)\n",
    "print(f'removed {num} short samples')\n",
    "dataset = [d for d in dataset if len(d['short_description']) > 40]\n",
    "random.Random(42).shuffle(dataset)\n",
    "\n",
    "# dataset = dataset[:50_000]\n",
    "texts = [i['short_description'] for i in dataset]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812e2184cf7f0a5",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ef284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/400.7 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/400.7 MB 435.7 kB/s eta 0:15:20\n",
      "     -------------------------------------- 0.1/400.7 MB 657.6 kB/s eta 0:10:10\n",
      "     -------------------------------------- 0.1/400.7 MB 656.4 kB/s eta 0:10:11\n",
      "     ---------------------------------------- 0.2/400.7 MB 1.1 MB/s eta 0:06:19\n",
      "     ---------------------------------------- 0.4/400.7 MB 1.4 MB/s eta 0:04:40\n",
      "     ---------------------------------------- 0.5/400.7 MB 1.7 MB/s eta 0:04:02\n",
      "     ---------------------------------------- 1.0/400.7 MB 2.9 MB/s eta 0:02:20\n",
      "     ---------------------------------------- 1.3/400.7 MB 3.4 MB/s eta 0:01:58\n",
      "     ---------------------------------------- 2.1/400.7 MB 4.7 MB/s eta 0:01:26\n",
      "     ---------------------------------------- 3.0/400.7 MB 6.2 MB/s eta 0:01:05\n",
      "     ---------------------------------------- 4.2/400.7 MB 8.1 MB/s eta 0:00:50\n",
      "      --------------------------------------- 5.3/400.7 MB 9.2 MB/s eta 0:00:44\n",
      "      -------------------------------------- 6.7/400.7 MB 10.7 MB/s eta 0:00:37\n",
      "      -------------------------------------- 8.0/400.7 MB 11.8 MB/s eta 0:00:34\n",
      "      -------------------------------------- 9.3/400.7 MB 13.0 MB/s eta 0:00:31\n",
      "      ------------------------------------- 10.5/400.7 MB 18.7 MB/s eta 0:00:21\n",
      "     - ------------------------------------ 11.8/400.7 MB 26.2 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 13.1/400.7 MB 28.4 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 14.5/400.7 MB 29.7 MB/s eta 0:00:13\n",
      "     - ------------------------------------ 15.6/400.7 MB 28.5 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 16.4/400.7 MB 27.3 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 17.8/400.7 MB 27.3 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 19.1/400.7 MB 27.3 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 20.5/400.7 MB 27.3 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 21.8/400.7 MB 27.3 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 23.1/400.7 MB 27.3 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 24.5/400.7 MB 27.3 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 25.8/400.7 MB 28.5 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 27.2/400.7 MB 28.5 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 28.5/400.7 MB 29.7 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 29.8/400.7 MB 28.5 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 31.2/400.7 MB 28.5 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 32.6/400.7 MB 28.4 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 33.9/400.7 MB 28.4 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 35.3/400.7 MB 28.4 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 36.6/400.7 MB 28.5 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 37.9/400.7 MB 28.5 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 39.2/400.7 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 40.5/400.7 MB 29.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 41.9/400.7 MB 29.8 MB/s eta 0:00:13\n",
      "     ---- --------------------------------- 43.2/400.7 MB 29.7 MB/s eta 0:00:13\n",
      "     ---- --------------------------------- 44.6/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 45.9/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 47.2/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 48.6/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 50.0/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 51.3/400.7 MB 29.8 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 52.6/400.7 MB 29.8 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 53.9/400.7 MB 28.5 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 55.3/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 56.5/400.7 MB 28.5 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 58.0/400.7 MB 28.4 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 59.2/400.7 MB 28.4 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 60.5/400.7 MB 28.4 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 61.8/400.7 MB 28.5 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 63.0/400.7 MB 28.5 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 64.4/400.7 MB 28.5 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 65.7/400.7 MB 28.5 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 67.1/400.7 MB 28.5 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 68.4/400.7 MB 28.4 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 69.7/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 71.1/400.7 MB 28.4 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 72.4/400.7 MB 29.7 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 73.8/400.7 MB 29.7 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 75.1/400.7 MB 29.7 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 76.5/400.7 MB 29.8 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 77.7/400.7 MB 29.8 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 79.1/400.7 MB 29.7 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 80.5/400.7 MB 29.7 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 81.8/400.7 MB 29.7 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 83.1/400.7 MB 28.4 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 84.4/400.7 MB 28.4 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 85.7/400.7 MB 28.4 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 87.0/400.7 MB 28.5 MB/s eta 0:00:12\n",
      "     -------- ----------------------------- 88.2/400.7 MB 29.7 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 89.4/400.7 MB 28.4 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 90.8/400.7 MB 28.5 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 92.1/400.7 MB 28.5 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 93.2/400.7 MB 28.5 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 94.5/400.7 MB 28.5 MB/s eta 0:00:11\n",
      "     --------- ---------------------------- 95.7/400.7 MB 27.3 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 96.9/400.7 MB 27.3 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 97.9/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     --------- ---------------------------- 99.2/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 100.3/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 101.5/400.7 MB 25.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 102.7/400.7 MB 25.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 103.9/400.7 MB 25.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 105.2/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 106.5/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     --------- --------------------------- 107.7/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     ---------- -------------------------- 109.0/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     ---------- -------------------------- 110.3/400.7 MB 27.3 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 111.5/400.7 MB 26.2 MB/s eta 0:00:12\n",
      "     ---------- -------------------------- 112.9/400.7 MB 28.4 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 114.2/400.7 MB 27.3 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 115.6/400.7 MB 28.5 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 116.9/400.7 MB 28.5 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 118.2/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 119.6/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 120.9/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 122.3/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 123.6/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 125.0/400.7 MB 29.8 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 126.3/400.7 MB 29.8 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 127.7/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 129.0/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 130.3/400.7 MB 29.7 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 131.6/400.7 MB 28.4 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 133.0/400.7 MB 28.4 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 134.3/400.7 MB 28.4 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 135.7/400.7 MB 28.5 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 137.1/400.7 MB 29.8 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 138.4/400.7 MB 28.5 MB/s eta 0:00:10\n",
      "     ------------ ------------------------ 139.8/400.7 MB 28.5 MB/s eta 0:00:10\n",
      "     ------------- ----------------------- 141.1/400.7 MB 28.5 MB/s eta 0:00:10\n",
      "     ------------- ----------------------- 142.5/400.7 MB 28.4 MB/s eta 0:00:10\n",
      "     ------------- ----------------------- 143.8/400.7 MB 28.4 MB/s eta 0:00:10\n",
      "     ------------- ----------------------- 145.1/400.7 MB 28.4 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 146.5/400.7 MB 28.5 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 147.9/400.7 MB 28.5 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 149.2/400.7 MB 28.5 MB/s eta 0:00:09\n",
      "     ------------- ----------------------- 150.5/400.7 MB 29.8 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 151.9/400.7 MB 29.8 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 153.2/400.7 MB 29.7 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 154.6/400.7 MB 29.7 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 155.9/400.7 MB 29.7 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 157.2/400.7 MB 29.7 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 158.5/400.7 MB 28.4 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 159.9/400.7 MB 29.7 MB/s eta 0:00:09\n",
      "     -------------- ---------------------- 161.2/400.7 MB 29.8 MB/s eta 0:00:09\n",
      "     --------------- --------------------- 162.6/400.7 MB 29.8 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 164.0/400.7 MB 29.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 165.3/400.7 MB 28.5 MB/s eta 0:00:09\n",
      "     --------------- --------------------- 166.7/400.7 MB 29.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 168.0/400.7 MB 29.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 169.3/400.7 MB 29.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 170.7/400.7 MB 29.7 MB/s eta 0:00:08\n",
      "     --------------- --------------------- 172.0/400.7 MB 29.8 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 173.4/400.7 MB 28.5 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 174.7/400.7 MB 28.5 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 176.0/400.7 MB 28.5 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 177.4/400.7 MB 28.5 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 178.8/400.7 MB 28.4 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 180.1/400.7 MB 28.4 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 181.3/400.7 MB 28.4 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 182.5/400.7 MB 28.5 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 183.7/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 184.8/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 186.0/400.7 MB 26.2 MB/s eta 0:00:09\n",
      "     ----------------- ------------------- 187.1/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 188.3/400.7 MB 26.2 MB/s eta 0:00:09\n",
      "     ----------------- ------------------- 189.5/400.7 MB 26.2 MB/s eta 0:00:09\n",
      "     ----------------- ------------------- 190.7/400.7 MB 26.2 MB/s eta 0:00:09\n",
      "     ----------------- ------------------- 191.9/400.7 MB 26.2 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 193.0/400.7 MB 25.1 MB/s eta 0:00:09\n",
      "     ----------------- ------------------- 194.1/400.7 MB 25.1 MB/s eta 0:00:09\n",
      "     ------------------ ------------------ 195.2/400.7 MB 25.2 MB/s eta 0:00:09\n",
      "     ------------------ ------------------ 196.5/400.7 MB 25.2 MB/s eta 0:00:09\n",
      "     ------------------ ------------------ 197.7/400.7 MB 25.2 MB/s eta 0:00:09\n",
      "     ------------------ ------------------ 199.0/400.7 MB 26.2 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 200.4/400.7 MB 26.2 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 201.7/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 203.0/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 204.3/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 205.5/400.7 MB 27.3 MB/s eta 0:00:08\n",
      "     ------------------- ----------------- 206.8/400.7 MB 28.5 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 208.2/400.7 MB 28.5 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 209.5/400.7 MB 28.5 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 210.8/400.7 MB 28.5 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 212.1/400.7 MB 27.3 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 213.5/400.7 MB 27.3 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 214.8/400.7 MB 29.7 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 216.1/400.7 MB 29.7 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 217.5/400.7 MB 29.7 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 218.9/400.7 MB 29.7 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 220.2/400.7 MB 29.8 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 221.5/400.7 MB 29.8 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 222.9/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     -------------------- ---------------- 224.2/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     -------------------- ---------------- 225.5/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     -------------------- ---------------- 226.9/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 228.3/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 229.6/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 231.0/400.7 MB 29.8 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 232.3/400.7 MB 29.8 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 233.7/400.7 MB 28.5 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 235.0/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 236.2/400.7 MB 28.5 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 237.6/400.7 MB 28.4 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 238.8/400.7 MB 28.4 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 240.2/400.7 MB 28.4 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 241.6/400.7 MB 28.5 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 243.0/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 244.3/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 245.7/400.7 MB 29.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 247.1/400.7 MB 29.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 248.3/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 249.7/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 251.0/400.7 MB 29.7 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 252.4/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 253.7/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 254.9/400.7 MB 28.4 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 256.3/400.7 MB 28.5 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 257.5/400.7 MB 28.5 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 258.9/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 260.0/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 261.4/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 262.6/400.7 MB 27.3 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 264.0/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 265.3/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 266.7/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 268.0/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 269.4/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 270.7/400.7 MB 29.8 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 272.0/400.7 MB 29.8 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 273.4/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 274.7/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 276.0/400.7 MB 29.7 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 277.3/400.7 MB 28.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 278.7/400.7 MB 28.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 280.0/400.7 MB 28.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 281.3/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 282.8/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 284.1/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 285.5/400.7 MB 28.5 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 286.8/400.7 MB 28.5 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 288.1/400.7 MB 28.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 289.3/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 290.7/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 292.0/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 293.3/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 294.7/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 296.1/400.7 MB 29.8 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 297.5/400.7 MB 29.8 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 298.8/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 300.2/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 301.5/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 302.9/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 304.1/400.7 MB 28.4 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 305.5/400.7 MB 29.7 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 306.8/400.7 MB 28.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 308.2/400.7 MB 28.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 309.5/400.7 MB 28.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 310.9/400.7 MB 28.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 312.2/400.7 MB 28.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 313.6/400.7 MB 28.4 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 315.0/400.7 MB 29.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 316.3/400.7 MB 28.4 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 317.7/400.7 MB 29.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 319.0/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 320.3/400.7 MB 29.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 321.5/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 322.8/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 324.2/400.7 MB 29.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 325.5/400.7 MB 29.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 326.9/400.7 MB 29.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 328.2/400.7 MB 28.4 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 329.5/400.7 MB 29.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 330.8/400.7 MB 28.4 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 332.1/400.7 MB 29.8 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 333.4/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 334.8/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 336.0/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 337.4/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 338.7/400.7 MB 28.4 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 340.0/400.7 MB 28.4 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 341.4/400.7 MB 28.4 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 342.7/400.7 MB 28.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 344.0/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 345.3/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 346.7/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 348.0/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 349.4/400.7 MB 28.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 350.7/400.7 MB 28.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 352.0/400.7 MB 28.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 353.3/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 354.6/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 355.9/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 357.2/400.7 MB 27.3 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 358.6/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 359.8/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 361.2/400.7 MB 27.3 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 362.5/400.7 MB 27.3 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 363.8/400.7 MB 29.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 365.2/400.7 MB 28.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 366.5/400.7 MB 29.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 367.9/400.7 MB 29.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 369.2/400.7 MB 28.5 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 370.6/400.7 MB 29.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 372.0/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 373.3/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 374.6/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 375.9/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 377.3/400.7 MB 28.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 378.5/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 379.9/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 381.2/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 382.5/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 383.9/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 385.2/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 386.5/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 387.9/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 389.2/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  390.5/400.7 MB 29.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  391.7/400.7 MB 28.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  393.0/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  394.4/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  395.7/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  397.1/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  398.4/400.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  399.9/400.7 MB 28.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  400.7/400.7 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 400.7/400.7 MB 14.5 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91e3f6fb5bfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom',\n",
    "                        prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "representation_model = KeyBERTInspired(random_state=42)\n",
    "# representation_model = TextGeneration('gpt2')\n",
    "\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    verbose=True\n",
    ")\n",
    "kw_model = KeyBERT(\n",
    "    model=embedder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc413c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_index = Index()\n",
    "topic_index = Index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde3eee90e177c",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd16ed5aa9a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of 171806 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5369/5369 [05:01<00:00, 17.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (171806, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f'Processing batch of {len(texts)} texts')\n",
    "embeddings: np.ndarray = embedder.encode(texts, show_progress_bar=True)\n",
    "print(f'Embeddings shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e17080d3b7144",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4984\\166523744.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m emb_index.add(embeddings)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4984\\2179279975.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, embeddings)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m add(self, embeddings: np.ndarray):\n\u001b[32m     50\u001b[39m         self.index = faiss.IndexHNSWFlat(embeddings.shape[\u001b[32m1\u001b[39m], \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         self.index.add(embeddings)\n",
      "\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\faiss\\class_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m d == self.d\n\u001b[32m    229\u001b[39m         x = np.ascontiguousarray(x, dtype=\u001b[33m'float32'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         self.add_c(n, swig_ptr(x))\n",
      "\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\faiss\\swigfaiss_avx2.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, n, x)\u001b[39m\n\u001b[32m   6976\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m add(self, n, x):\n\u001b[32m-> \u001b[39m\u001b[32m6977\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss_avx2.IndexHNSW_add(self, n, x)\n",
      "\u001b[31mMemoryError\u001b[39m: std::bad_alloc",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3720\u001b[39m, in \u001b[36mInteractiveShell.run_code\u001b[39m\u001b[34m(self, code_obj, result, async_)\u001b[39m\n\u001b[32m   3718\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   3719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3720\u001b[39m         result.error_in_exec = \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   3721\u001b[39m     \u001b[38;5;28mself\u001b[39m.showtraceback(running_compiled_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3722\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "emb_index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baee4d80a3954be",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = train_topic_model(texts, topic_model, embeddings)\n",
    "\n",
    "topic_labels = topic_model.generate_topic_labels()\n",
    "\n",
    "num_topics = len(topic_model.get_topic_info())\n",
    "print(f'Number of topics: {num_topics}')\n",
    "print(f'Topics: {topic_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c5563f139e1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist_matrix, _ = topic_model.approximate_distribution(texts)\n",
    "print(f'Topic distribution shape: {topic_dist_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1e5ece160e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index.add(topic_dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9401e653036ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m entities = \u001b[43mextract_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mextract_entities\u001b[39m\u001b[34m(texts, nlp)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extract entities from text documents\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m res = []\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentities\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43ment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43ments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNOT_ALLOWED_ENTITIES\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentity_counter\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\language.py:1622\u001b[39m, in \u001b[36mLanguage.pipe\u001b[39m\u001b[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[39m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1621\u001b[39m         docs = pipe(docs)\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:252\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:343\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:275\u001b[39m, in \u001b[36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:814\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.set_ents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mo\\Desktop\\projects\\recommendation\\.venv\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:127\u001b[39m, in \u001b[36mspacy.tokens.doc.SetEntsDefault.values\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\enum.py:806\u001b[39m, in \u001b[36mEnumType.__members__\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[33;03m    Return the number of members (no aliases)\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m._member_names_)\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m \u001b[38;5;129m@bltns\u001b[39m.property\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__members__\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    808\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    809\u001b[39m \u001b[33;03m    Returns a mapping of member name->value.\u001b[39;00m\n\u001b[32m    810\u001b[39m \n\u001b[32m    811\u001b[39m \u001b[33;03m    This mapping lists all enum members, including aliases. Note that this\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[33;03m    is a read-only view of the internal mapping.\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MappingProxyType(\u001b[38;5;28mcls\u001b[39m._member_map_)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "entities = extract_entities(texts, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d291c53fcbf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_matrix = extract_keywords(texts, kw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c6adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = extract_keywords(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae147900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the solution to finding more time for what matters while still succeeding at work?  There is no magic formula, but I would like to share three basic ideas for how we can begin to tame our calendars and achieve better balance.',\n",
       " ['share three basic ideas',\n",
       "  'achieve better balance',\n",
       "  'would like',\n",
       "  'still succeeding',\n",
       "  'magic formula',\n",
       "  'work',\n",
       "  'time',\n",
       "  'tame',\n",
       "  'solution',\n",
       "  'matters',\n",
       "  'finding',\n",
       "  'calendars',\n",
       "  'begin'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3], keywords[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b6252ee53b092",
   "metadata": {},
   "source": [
    "# Add to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62d276fb2b6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_graph = ContentGraph()\n",
    "content_store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31db60a5c74c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    content_store[i] = Content(\n",
    "        content_id=str(i),\n",
    "        text=text,\n",
    "        embedding=embeddings[i],\n",
    "        entities=entities[i],\n",
    "        topic_dist=topic_dist_matrix[i],\n",
    "        topic=topics[i],\n",
    "        keywords=set(keywords_matrix[i]),\n",
    "        centrality=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814cb514d5153617",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_store['178234']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d971f0917759b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_store[np.int64(178234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da43ea63df9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_per_item = emb_index.get_items_by_index([c.embedding for c in content_store.values()], 100)\n",
    "candidates_per_item_2 = topic_index.get_items_by_index([c.topic_dist for c in content_store.values()], 100)\n",
    "for i, content in enumerate(content_store.values()):\n",
    "    candidate_ids = candidates_per_item[i] + candidates_per_item_2[i]\n",
    "    candidate_ids = list(set(candidate_ids))\n",
    "    candidates = [content_store[c] for c in candidate_ids]\n",
    "    add_article_to_graph(content, candidates, content_graph)\n",
    "\n",
    "# calculate centrality for each item\n",
    "centrality = content_graph.get_centrality()\n",
    "for content_id, centrality_score in centrality.items():\n",
    "    content_store[content_id].centrality = centrality_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb520bc3335cc9",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119d2c06e8cf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d38e8cba8366c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = []\n",
    "cluster_id = 365\n",
    "for content in content_store.values():\n",
    "    if content.topic == cluster_id:\n",
    "        cluster.append(content)\n",
    "\n",
    "for content in cluster:\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7adb53ddb404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = {}\n",
    "user_id = \"user1\"\n",
    "\n",
    "if user_id in user_profiles:\n",
    "    del user_profiles[user_id]\n",
    "\n",
    "user = create_user(user_id, user_profiles)\n",
    "history = [\n",
    "    'https://www.huffingtonpost.com/entry/cats-family-babysitter_us_5b9dd60ee4b03a1dcc8d8d54',\n",
    "    'https://www.huffingtonpost.com/entry/a-wedding-theme-featuring_us_5b9deb0ce4b03a1dcc8eb7ea'\n",
    "]\n",
    "for h in history:\n",
    "    c = content_store[h]\n",
    "    history = [content_store[i] for i in user.interaction_history]\n",
    "    record_interaction(user_id, c, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58b8cdb990c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations_user(\n",
    "    user,\n",
    "    strategy_weights={\n",
    "        'embedding': 0,\n",
    "        'topic': 0,\n",
    "        'entity': 0,\n",
    "        'explore': 0\n",
    "    },\n",
    "    candidates=content_store.values(),\n",
    "    num_recommendations=7\n",
    ")\n",
    "\n",
    "hist = \"\\n\".join(str(u) for u in user.interaction_history)\n",
    "print(f'[DEBUG] user history: \\n{hist}')\n",
    "print(f'[DEBUG] user topic interests: {user.topic_interests}')\n",
    "print(f'[DEBUG] user entity interests: {user.entity_interests}')\n",
    "print(\"\\nRecommendations:\")\n",
    "history = [content_store[i] for i in user.interaction_history]\n",
    "for rec_id, score in recommendations:\n",
    "    content = content_store[rec_id]\n",
    "    print(f'\\n- id: {rec_id}, score: {score} summary: {content.text}')\n",
    "    print(f\"  Why: {'; '.join(explain_recommendation_user(user_id, rec_id, history))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686157787432a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: look into keyword extraction using keybert\n",
    "# todo: add user interests\n",
    "# # topic reduction\n",
    "#\n",
    "# topic_model.reduce_topics(docs, nr_topics=30)\n",
    "#\n",
    "# # Access updated topics\n",
    "# topics = topic_model.topics_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
